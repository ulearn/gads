Usage:

node crawl-docs.js https://developers.google.com/google-ads/api/docs --depth 2 --save googleads-index.json


This builds a compact JSON index of every pageâ€™s title + first 2 000 chars.
Claude Code (or any script) can then search that offline file to find the right section instantly.

ğŸ§  3ï¸âƒ£ Heuristic strategy (â€œgeneral RTFM brainâ€)

When you run any script that needs docs:

Check cache first (/docs-index/*.json).S

If not present, crawl the root URL (depth â‰¤ 2 or 3).

Summarize page titles â†’ headings â†’ URLs.

Look up relevant endpoint pages by keyword.

Fetch only those pages in full when you actually need details.

That gives Claude Code a pseudo-â€œsearch engineâ€ view of any APIâ€™s manual without uncontrolled crawling.

âš™ï¸ 4ï¸âƒ£ Optional enhancements

Add robots.txt respect (donâ€™t crawl banned paths).

Limit rate (await sleep(500) between requests).

Use cheerio to extract <code> or <pre> blocks only (for endpoint examples).

Persist a sitemap summary:

{ "GET /campaigns": "https://...", "POST /mutate": "https://..." }


â†’ Claude Code can consult this index for field names later.

âœ… TL;DR

Axios fetcher = read one page.
Mini crawler (cheerio + queue + URL checks) = read entire manual.
Cache & index results â†’ Claude Code can â€œRTFMâ€ across hierarchies intelligently.